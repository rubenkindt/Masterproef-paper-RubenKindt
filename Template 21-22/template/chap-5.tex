\chapter{Implementation}
\label{cha:impl}
\label{impl:Intro}
In this chapter we will discuss how we build our fuzzers, what issues we had to circumvent and how we did that. With first starting off how we got our seeds to fuzz upon.
\todo{Proofread entire chapter}

\section{Software versions used}
Throughout this paper we used CPMpy version V0.9.9 (commit \href{https://github.com/CPMpy/cpmpy/commit/e79b3afedc934a9437c2ddb3a9f54d7e2d7bd3ee}{e79b3af}) unless specified otherwise. This version was chosen simply because it was the latest release version at the time of testing the first technique. All techniques were developed in Python3.8, the MiniZinc solvers came with minizinc-python \footnote{\url{https://github.com/MiniZinc/minizinc-python}} release version 0.7.0 (commit \url{https://github.com/MiniZinc/minizinc-python/commit/a195cf63fcfbc98665d70ab64efb5424db25bd7e}{a195cf6}). For the proprietary solver Gurobi \footnote{\url{https://www.gurobi.com/}} we used version 9.5.2 with an academic license. 
%problems Gurobi outputting Not_run this was a trial problem, with the academic version does not occure
Originally, we did try to utilize the trial version to ease possible reproducibility, but the restrictions on the complexity of the problems became a hindrance to fast which resulted us moving to the academic license.

\todo{solver versions}


\section{Obtaining seeds}
\label{impl:obtainingSeeds}
As discussed in a previous section (section \ref{fuzzing:generationMutation}) generating new inputs is significantly harder than mutation, but with the latter one we require a diverse set of seed files. Fortunately, the CPMpy team made a lot documentation and examples on how to model problems in their language. Ranging from easy examples to teach the language to advanced examples in order to showcase certain features. 
At the moment of writing most examples are found in the main branch and some extras can be found the "csplib" branch \footnote{\url{https://github.com/CPMpy/cpmpy/tree/csplib}} waiting to be merged with the main branch. We downloaded a copy of that branch on Tuesday 27th of September to be used as future seed files. 

A second source of seeds files came from Hakan Kjellerstrand a retired software developer and independent researcher from Sweden which was found while reading \cite{18bleukx2022model} and got recommended by Ignace Bleukx. He has a big repository \footnote{\url{https://github.com/hakank/hakank/tree/master/cpmpy}} full of problem models which he solves in multiple ways, including CPMpy. We obtained a copy of all his CPMpy examples on Tuesday 27th of September to top off our collection of future seed files.


%We also made sure that the seed files were able to be run in less then 60 sec either by reducing the amount of solver calls per example of in extreme situations removed some examples
After that we ran all examples to test that the base examples do not crash on their own and noticed that most examples run in less then a minute. The handful of examples that did run extremely long we did leave out or simplified to gain a speed up while solving of them. A last change we did to the future seed files is extracting the constraints from each example, we did this for a couple of reasons some files had a loop around the solve instruction combined with small changes or had multiple problems in one file. In order to extract these constraints we temporary modified CPMpy to extract the created model, constraints included, each time solve was called. This resulted in over nine thousand problem models which we will use as our seed files.


We extracted our seeds twice, a first time where we extract our model without any flattening of the constraints and a second time where we did flatten the constraints. While building up a model of the problem CPMpy allows for arbitrary complex compositions of constraints resulting in a nested tree of constraints. But not all solvers allow this nested tree as described by the documentations of CPMpy. It is for that reason that CPMpy flattens the constraints to what they call 'normal forms' as the similar definition of SAT but with a disclaimer that this does not exits to their knowledge with which we agree with. With this flattened form CPMpy can directly call the solvers or do some solver specific transformations on the flattened constraints to then send it to the solver. The reason we extracted our seeds with and without a flattening process is that \todo{aanvullen na vraag} a flattened version and the reason we did is without will become clear in the next section.



\section{Modifying STORM}
\label{impl:modifyingSTROM}
Our first technique of finding bugs is heavily based on STORM which we shortly discussed before in section \ref{fuzzing:testingWithFuzzers}, which we altered to be able to find bugs in constraint programming languages and specifically CPMpy. We started with downloading STORM from the repository \footnote{\url{https://github.com/Practical-Formal-Methods/storm}} on Tuesday 27th September.
%https://github.com/Practical-Formal-Methods/storm/commit/55d091624523a0544112ffc339fe81103b3daa2b
The original plan was to convert our seeds to FlatZinc using the MiniZinc API provided by CPMpy to then convert that to SMT-LIB \cite{72bofill2010system} using Miquel Bofill et al.'s fzn2smt-tool to then be able to use STORM like it was build originally. Unfortunate and a bit predictable, this way of working did not work out. On top of fzn2smt being more then a decade old, the multiple transformation layers that could introduces conversion bugs and the unclear way back from SMT-LIB prevented this path from being investigated by us.
Therefore, we decided to refactoring STORM in order to fit CPMpy, in order to do this we need to rewrite the detection, labeling and construction of (sub)constraints. This refactoring did come with some downsides, some features of STORM we did not need no longer work like incremental solving or the input obfuscation that was build-in. A bigger downside came with the refactoring of the negation function of STORM, as CPMpy is still in active development is not always available and that was felt while trying to negate global functions. I.e. when trying to invert (sub)constraints which include AllDifferent([var1, var2, var3]) using CPMpy, it crashed. This is of course a bug (more specific not yet implemented) in CPMpy, but vital to the fuzzer. So here we had the choice of adding the missing negation of global functions to CPMpy or to limit our fuzzer to not use the missing features. We choose to limit the fuzzer, since we are trying to detect bugs in CPMpy with different tools and extending the language ourselves goes out of cope of this dissertation. 

%remove?
%The resulting limitation on our fuzzer only influences the speed of generating new constraints and it can theoretically now get stuck but this has not happened yet, so we believe it to be a acceptable limitation.


We gave only non-flattened inputs to this solver since STORM used a recursive process to get all subformulas because we wanted to change as little as possible to the inner workings of STORM. We therefore hijacked flatten process of CPMpy to also return all subformulas before returning the flattened constraints. This gave access to the more convoluted subformulas to use in the next steps of STORM before they are flattened. This flattening process was done before any modifications where made, so in the yes of the fuzzer it got flattened seeds but with the knowledge of some more complex constraints just like STORM does.
\todo{image of this process?}


%problems limited by global fucntions of used seeds (somewhat fine),
%only 'and' and 'not' combinations Just like STORM

%DUMB ideas:
%Translating seeds from solver X to solver Y 
%option 1 hardcode default solver of CPMpy to Y, less good modifying the language is something we want to avoid. May also not work when solver is hard coded in the seed.
%Option 2 interpret the seed and make changes so that the solver Y is run. Bit trickier as you cannot see the difference between model.solve() and solver.solve() because model and solver are variables.
%
%nested functions with global function inside are giving problems

\section{Differential testing}
With differential testing we step a bit further way from the fuzzing world but not that far since this can be integrated as an check to see in the fuzzer. With the previous technique this was not needed due knowing the correct solution in advance. The way this tester is written is quite easy we test a given input on multiple solver and search if there is any difference in outputs or on the number of solutions provided.
%ony 2 solvers
However after finishing, we discovered that only two solver, namely "ortools" and "gurobi"-solvers, are able to search all solutions for problem models that contain global function. More are able to find all solutions for SAT problems, but that is not the main objective of this dissertation. This limitation to only two solvers results in only able to compare two different implementations and has a risk of overlapping bugs. Preferably, we would have three or more solvers to be able to compare between them and also able to automatically show us which of the solver is likely to be wrong. In the future more solvers will be available with the capability of finding all solution withing CPMpy, but right now these tests are preformed with two solvers. 

While searching for all possible solutions for a given problem over multiple solvers gave us more data to compare in between solvers the restrictions mentioned limit the testing of finding all solutions. However, when we look at only searching for one solution for a given example we have always have more then enough solver to compare between. Most of the times 14 solvers where compared and we never gotten lower then 6 solver, this variation is due to the features used in the given problem the amount of solvers changed. If a problem happened to not use any global functions the SAT-solvers may be able to solve them allowing us to compare between up to 25 solvers. Given that we only search after a single solution we are limited in the what we can compare. For example, if solver A says that it found a satisfiable problem with the solution having values A and solver B says the same but with values B (with A $\neq$ B) the only thing we can compare is that both solver output the same "satisfiable". Since both solution A and B can be different solutions to the problem.


\section{metamorphic testing}


\section{creation of sat and unsat formulas}
see paper 43 p 4
Semantic Fusion
\cite{43YinYang}
een ref to chapter 2

\section{Conclusion}
\label{impl:conclusion}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
