\chapter{Detecting crucial parts in inputs}
\label{cha:3}
\label{cha:3:intro}
\todo{algorithms?} paper 5 has nice simplification to isolation delta deb, called 'ddmin()'
When we detect that the PUT crashes, wrongly satisfies, wrongly not satisfies or hangs on a given input we now want to know why it does that. What causes this unwanted output and what line the bug occurs. With crashes, a stack trace and some luck this could be easy, but when the crash is not main perpetrator or we get an other unwanted output the developer my need to debug deep into the code to find the bug. This with a potential large input could be a tedious and long assignment, for this reason we would like to know what parts of the input are related to the bug. We will discover this further in this chapter, starting with \todo{plug in sections}

\section{Deobfuscating inputs}
\label{cha:3:Deobfuscating}
When receiving a big input the chance of having parts unrelated to the bug is almost guaranteed, we will call them (unintentionally) obfuscated inputs. Deobfuscating those takes a lot of try and error to see if the bug is still there\cite{bookZellerwhyProgramsFail} or having to walk through the execution to find the bug. Both take a while if we want to go to absolute minimal inputs, but for developers it is not needed to go to that extreme. As long as we take the bulk of the unrelated parts of inputs away it will help the developer to find the bug faster. With these techniques we can also group similar bugs and deduplicate error reports (more on that later) which is also fairly useful information for developers.
\subsection{Simplifying}
\label{cha:3:Simplifying}
to find crucial parts of inputs is often done with simplification or Isolation, simplification is the technique where we remove parts of a failing input and check if it still fails and it often called "delta-debugging"\cite{bookZellerwhyProgramsFail} which belongs to the divide-and-conquer family of algorithms \cite{2FuzzingAndDeltaDebuggingSMTSolvers}. When it is no longer possible to remove any part of the input we have obtained an input where all parts are needed to expose the bug. This input is at the same time also the shortest possible input to trigger this bug, making finding the bug easier than in the original input filled with unrelated parts.
\subsection{Isolation}
\label{cha:3:Isolation}
Another technique, isolation, is explained by Andreas Zeller et al. 
\cite{5zeller2002simplifyingIsolatingFailure-inducing} this is a technique where instead of minimizing the input we try to find the smallest difference between an input that shows the bug versus an input that does not show the bug. This with the advantage that no matter if we find the bug or not the difference will diminish, either the maximum input will shrink or the minimum input will grow. This technique brings extra complexity with the tracking of multiple inputs and the maximal input could take longer to run due to its size, but according to Andreas Zeller et al. this is the faster one to the two. Figure
\ref{fig:simplificationIsolation} shows the difference between simplifying and isolation, both finding the critical part of the input, with simplification the critical part is indicated by the last test in the figure while with isolation it it the differenc of the passed and failed test.
\begin{figure}
	\centering
	\includegraphics[width=1.0\textwidth]{images/simplificationIsolation}
	\caption{Deobfuscating inputs based on simplification (left) and isolation (right) on the same input. The '*' indicates that the result is already known and does not need to be recalculated. Figure based on an illustrations found in\cite{bookZellerwhyProgramsFail}.}
	\label{fig:simplificationIsolation}
\end{figure}

\subsection{Alternative approach}
An alternative approach by Alexandra Bugariu and Peter M\"uller\cite{9bugariu2020automaticallyTestingStringSolvers} is to forgo the need of deobfuscating inputs by generating inputs "small by construction". Or by fuzzing in a similar way to when the bug was found as done by Muhammad Numair Mansur et al\cite{1mansur2020detecting} and by adding a small limitation getting a smaller input.

\section{The precision effect}
\label{cha:3:PersisionEffect}
With one caveat to take in mind we need to be careful to still find the same bug and to not change a null pointer dereference bug to a parser related bug. This, as discussed in the previous chapter, is due to some bugs being more important than others. In a paper by Andreas Zeller and Ralf Hildebrandt \cite{5zeller2002simplifyingIsolatingFailure-inducing} talk about this exact problem which they called "the Precision Effect". Sometimes this is not a problem for example when we are trying to find all possible bugs and will rerun the fuzzer after each incremental improvement or the situation where a deeper bug turns into another deep bug. But overall we try to avoid this effect, which can be done with the techniques we will talk about in section \ref{cha:3:Deduplication}.
\section{What size to change}
\label{cha:3:Chucksize}
Another thing we glossed over is the chuck sizes to remove while trying to find the critical parts of the inputs. The previous seen techniques will work well on the original fuzz testing Miller et al.\cite{4originalFuzzingUnixUtils} worked on since those random generated symbols where independent from each other. When testing something more complex words like "while" or "float" we no longer can split on all possible places, since the input would most likely no longer parse. The same for splitting size, in the exact middle for figure \ref{fig:simplificationIsolation} we conveniently took one-eighth of the input as the chuck sizes for the ease of the example. For performance reasons we hope we can keep our chuck sizes as big as possible to be able to discard larger unrelated parts of the inputs. but when this is not possible we will need to decrease the granularity of the chuck sizes. To be able to find the critical parts of an input of the form "XXooXooXXoo" (with 'o' being the critical parts and the 'X' being unrelated to the bug) we should always search further with same granularity while the removed parts are already removed until all options with that granularity searched\cite{bookZellerwhyProgramsFail}. This will make sure that we eliminate all unrelated parts with the specific granularity and get "ooXoooo". 

We could also apply some techniques seen in section \ref{cha:2:InputStructure} where we discussed the creation of randomly and smarter created inputs. Instead of removing (hopefully) unrelated parts based purely on where the part sits in the input. We can use knowledge of the input structure or knowledge of the PUT to guide us in the removal\cite{bookZellerwhyProgramsFail}, both lexical (the meaning of words) and syntactical knowledge (the meaning of combinations of words) can be used to help us in deobfuscating inputs. Where syntactical knowledge would help us remove the most since it is the bigger of the two.

\subsection{Preserving satisfiability}
With techniques as mentioned in section \ref{cha:2:handelingOracelproblem}, satisfiable by construction will need to take in mind the extra complexity of preserving the ground truth when deobfuscating inputs. either we can apply Muhammad Numair Mansur et al.'s\cite{1mansur2020detecting} technique of trying to fuzz the same bug again, but with less symbols or as Robert Brummayer and Armin Biere\cite{2FuzzingAndDeltaDebuggingSMTSolvers} did use other SMT solvers.
\todo{NEEDS example}

\section{Deduplication}
\label{cha:3:Deduplication}
Another thing to notice is that multiple inputs could prompt the same bug from occurring, these inputs could be similar but don't have to be. With simplifying the input we should be able to detect exact copies, but depending on the simplification's time complexity other techniques could be better with similar results. In case where we would have access to stack traces (via crashes or hanging PUT's) we could differentiate the bugs on basis of the hash of multiple lines from the backtrace sometimes even numerous hashes per input. this technique is called stack backtrace hashing and is quite popular according to Valentin J.M. Man\`es et al\cite{13manes2019survey}. Another technique talked about in that paper, is looking at the code coverage generated by the inputs where we use the executed path (or hash of it) is used as a fingerprint of the inputs. A technique, used by Microsoft\cite{36semanticsAwareDeduplicationRETracer} is called semantics based deduplication, where in stead of back track use memory dumps to hopefully find the origins of bugs. This use of dumps is less ideal due to traces having more information, but the latter is not always possible due to the performance overhead and privacy causes as specified in the paper. A last technique would be looking at the bug description left by a manual bug reports by the user, although this dependence on the quality of the bug reports and is most likely poorly automatable. None of the techniques mentioned above are perfect: with stack backtrace hashing you could find to many false positives or false negatives depending on the depth taken from the stack, with coverage some inputs will generate extra function calls and the semantics based deduplication are limited to X86 or x86-64 code with the binary file and the debug information. Neither of these techniques work with black box fuzzing unfortunately.

\section{Conclusion}
\label{cha:3:Conclusion}
\todo{Conclusion}
The final section of the chapter gives an overview of the important results
of this chapter. This implies that the introductory chapter and the
concluding chapter don't need a conclusion.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
