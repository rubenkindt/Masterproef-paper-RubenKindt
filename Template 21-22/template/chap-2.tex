\chapter{Fuzzing}
\label{cha:2:fuzzing}
A chapter is a logical unit. It normally starts with an introduction, which
you are reading now. The last topic of the chapter holds the conclusion.
\todo{intro chapter 1}

%if inf seed set, reducing it is better with the following technique (the min seed subset without any code coverage loss) \cite{14rebert2014seedselecting}
%seed selection problem, inf seeds which one do we use
%other fuzzers internal workings
%Chapter around fuzzing
%	\cite{mathis2019parser} states 3 optins traditional, stochastic and syntax driven.
%avoiding inf loops -> timeouts
%
%subsection about (psuedo-)randomness


\section{History} \todo{fix title}
The rise of fuzzing came when Miller gave a classroom assignment\cite{21FuzzingAssignment} in 1988 to his computer science students to test Unix utilities with randomly generated inputs with the goal to break the utilities. Two years later in December he wrote a paper\cite{4originalFuzzingUnixUtils} about the remarkable results. That more than 24\% to 33\% of the programs crashed.
In the last thirty years the technique of fuzzing has changed significantly and various classifications have come forward\cite{12Fuzzingasurvey}\cite{13manes2019survey}. Three most used classifications are: 
how does the fuzzer create input, how well is the input structured and does the fuzzer have knowledge of the program under test (PUT)?


%generation or mutatuion
%black, gray white box
%lexical, sematic, constraint, random

\section{Generation of mutation}
A fuzzer can construct input for a PUT in two ways, it can generate input itself or it can take an existing input and modify it, which are often called seeds. While Generation is more common than modifying when it comes to in smaller inputs the opposite is true for larger inputs. This is cause by the fact that generating semi-valid input becomes a lot harder the longer the input becomes. For example generating the word "Fuzzing" by uniformly random sampling using ASCII, has a chance of one in $5*10^{14}$ of happening, making this technique infeasible when we want to generate bigger semi-valid inputs. With mutation we can start of with larger and valid input and make modifications to create semi-valid inputs. With this last technique the diversity of the seeding inputs does become quite important. Ideally we would have an unlimited diverse set of inputs, but due to limited computation and available inputs we sometimes need to take a subset. In a paper by Rebert et al. \cite{14rebert2014seedselecting} they say that seed selection algorithms can help and compare random seed selection to the minimal subset of seeds with the highest code coverage among other seed selection algorithms. 

\section{Input structure}
%lexical, sementical, constraint or random
While we have discussed the bigger scope on how inputs are created, let us go into more detail. As we have seen before fuzzing started mainly with Miller's assignment, that random generation of inputs falls under 'dumb' fuzzing. Due to only seeing the input as one long list of strings with no knowledge of any substrings. This technique can be applied to mutation as well, compared to only adding with generation here we also additionally remove or change randomly selected symbols of the seed with new symbols. 
We can create three types of inputs: valid, semi-valid and nonsense input. With nonsense input we will be testing the parser almost exclusively, either it crashes the parser or the parser will say invalid and the PUT will stop running. With semi-valid input we hope to be as close as possible to valid input to be able to explore the PUT but to catch an edge cases and crash.

A smarter technique is referred to one which has knowledge about the structure inputs can have or should have. This increases the chance of inputs passing the parser, being able to test the deeper parts of the PUT and as such covering more of the PUT. This with an increased complex fuzzer. We can build a 'smart' fuzzer by adding knowledge about keywords, making it a lexical fuzzer, adding knowledge about syntax, for example all parentheses needs to be matched. Directed fuzz testing does fit in this category as well but is only possible in a white box environment, more on that later.

\section{Black, grey or white box fuzzing}
Now that we have discussed adding knowledge of inputs to the fuzzer, we can also add knowledge about the PUT to the fuzzer. Which brings us to black, grey and white box fuzzing. With black box fuzzing we have no knowledge about the working of the PUT and we treat the PUT as a literal black box, we present out input and we look at what comes out of it. With only minimal information the fuzzer then tries to improve its input creation. This was also the technique that Miller (unknowingly) used.
 
With grey box fuzzing is usually comes with tools that give indirect information to the fuzzer, tools like: code coverage measurements, timings, types of errors and more.

Then we arrived at white box testing, with this technique fuzzers can know the source code and can adjust there inputs to fuzz specific parts of the code at a higher cost due to having to reverse engineer the path to specific edge cases. You may may have already suspected, white box fuzzing has more knowledge and can find more bugs per input but creating those inputs take more time compared to black box fuzzing.

The differentiation between black, grey and white box fuzzing is not clear cut, most people would agree that white box fuzzing has full knowledge about the PUT, including the source code, that grey box fuzzing has some knowledge about the PUT and that black box fuzzing has little to no knowledge about the PUT. Going into more detail all we can say is that it is no longer a black-and-white situation and that the lines become more fuzzy. \todo{is this pun allowed?}


\section{Fuzzing small programs} \todo{fix title}
\section{Fuzzing big programs}

\subsection{clasical fuzzing (for some original cite's)}
\subsection{AFL++}
\subsection{KLEE}
\subsection{ClusterFuzz}
\subsection{STORM (startingpaper)}
A novel way of approaching this oracle problem is by Alexandra Bugariu and Peter M\"uller in "Automatically testing string solvers"\cite{9bugariu2020automaticallyTestingStringSolvers} where they know the (un)satisfiability of the formulas by the way of construction, called (un)sat by construction.  For formulas that satisfy they generate trivial satisfiable formulas and then by satisfiability preserving transformations create more complex formulas. And unsatisfiable formulas they use $\neg$ A $\land$ A', with A' being a equivalent formula of A, to create the trivial unsatisfiable formulas. To increase the complexity of those trivial formulas, they again depend on satisfiability preserving transformation.
This technique has also been applied to SMT solvers by Muhammad Numair Mansur at al.\cite{1mansur2020detecting}

\subsection{types of bugs} \cite{1mansur2020detecting}

\subsection{oracle (what is a crash)}
%oracle problem? see holy grail
%somewhere a ref to later chapter input simplification (minimisation differantioation)

\section{Conclusion}
The final section of the chapter gives an overview of the important results
of this chapter. This implies that the introductory chapter and the
concluding chapter don't need a conclusion.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
