\chapter{Fuzzing}
\label{cha:2:fuzzing}
A chapter is a logical unit. It normally starts with an introduction, which
you are reading now. The last topic of the chapter holds the conclusion.
\todo{intro chapter 1}

%if inf seed set, reducing it is better with the following technique (the min seed subset without any code coverage loss) \cite{14rebert2014seedselecting}
%seed selection problem, inf seeds which one do we use
%other fuzzers internal workings
%Chapter around fuzzing
%	\cite{mathis2019parser} states 3 optins traditional, stochastic and syntax driven.
%avoiding inf loops -> timeouts
%
%subsection about (psuedo-)randomness


\section{History} \todo{fix title}
The rise of fuzzing came when Miller gave a classroom assignment\cite{21FuzzingAssignment} in 1988 to his computer science students to test Unix utilities with randomly generated inputs with the goal to break the utilities. Two years later in December he wrote a paper\cite{4originalFuzzingUnixUtils} about the remarkable results. That more than 24\% to 33\% of the programs tested crashed.
In the last thirty years the technique of fuzzing has changed significantly and various innovations have come forward. The three\cite{12Fuzzingasurvey}\cite{13manes2019survey} most made classifications are: how does the fuzzer create input, how well is the input structured and does the fuzzer have knowledge of the program under test (PUT)?

\section{Generation or mutation}
A fuzzer can construct input for a PUT in two ways, it can generate input itself or it can take an existing input and modify it, those original inputs are often called seeds. While Generation is more common than modification when it comes to in smaller inputs the opposite is true for larger inputs. This is cause by the fact that generating semi-valid input becomes a lot harder the longer the input becomes. For example, generating the word "Fuzzing" by uniformly random sampling ASCII, has a chance of one in $5*10^{14}$ of happening, making this technique infeasible when we want to generate bigger semi-valid inputs. With mutation we can start of with larger and already valid input and make modifications to create semi-valid inputs. With this last technique the diversity of the seeding inputs does become quite important. Ideally we would have an unlimited diverse set of inputs, but due to limited computation and available inputs we sometimes need to take a subset. In a paper by Rebert et al. \cite{14rebert2014seedselecting} they propose that seed selection algorithms can improve results and compare random seed selection to the minimal subset of seeds with the highest code coverage among other algorithms. 

\section{Input structure}
%lexical, sementical, constraint or random
While we have discussed the bigger scope on how inputs are created, let us go into more detail. As we have seen before fuzzing started mainly with Miller's classroom assignment, this random generation of inputs falls under 'dumb' fuzzing. Due to only seeing the input as one long list of strings with no knowledge of any sub-strings. This technique can be applied to mutational fuzzing as well. Compared to only adding symbols with generational fuzzing here we also remove or change randomly selected symbols. 
We can create three types of inputs: valid, semi-valid and nonsense input. With nonsense input we will almost be exclusively testing the syntactic stage of the PUT, often called the parser. Either the input crashes the parser or the parser will return invalid and the PUT will stop running. With semi-valid input we hope to be as close as possible to valid input to be able to explore the PUT beyond the parser and to catch an edge cases here.

A smarter technique is referred to one which has knowledge about the structure inputs can have or should have. This increases the chance of inputs passing the parser, being able to test the deeper parts of the PUT and as such covering more of the PUT's code. At the cost of needing a more complex fuzzer. We can build a 'smart' fuzzer by adding knowledge about keywords, making it a lexical fuzzer, adding knowledge about syntax, for a syntactical fuzzer which can for example match all parentheses. Directed fuzz testing does fit in this category as well but it is not possible in a black box environment, more on that later.

\section{Black, gray or white box fuzzing}
Now that we have discussed adding knowledge of inputs to the fuzzer, we can also add knowledge about the PUT to the fuzzer. Which brings us to black, gray and white box fuzzing. With black box fuzzing we have no knowledge about the inner working of the PUT and we treat the PUT as a literal black box, we present out input and we look at what comes out of it. And with this minimal information the fuzzer then tries to improve its input creation. 
Compared to black box fuzzing gray box fuzzing usually comes with tools that give indirect information to the fuzzer, tools like: code coverage measurements, timings, types of errors and more.
And as you may have predict white box testing is the term used when the fuzzer as much as possible. It will have access to the source code and can adjust their inputs to fuzz specific parts of the code. This at a higher cost due to having to reverse engineer the path to specific edge cases, meaning that white box fuzzing can find more bugs per input but creating those inputs take more time compared to black box fuzzing.
The differentiation between black, gray and white box fuzzing is not clear cut, most people would agree that white box fuzzing has full knowledge about the PUT, including the source code, that gray box fuzzing has some knowledge about the PUT and that black box fuzzing has little to no knowledge about the PUT. Going into more detail all we can say is that it is no longer a black-and-white situation and that the lines become fuzzy. \todo{is this pun allowed?}


\section{Classification of some fuzzers}
Now that we know how we can classify fuzzers let us apply this to existing fuzzers to see how they work. For starters Miller's original work, which we discussed earlier, was random generation based black box fuzzing. His later work in 1995 on more UNIX utilities and X-Windows servers\cite{26miller1995fuzzrevisited}, his work in 2000 on Windows NT 4.0 and Windows 2000\cite{24MillerWindows} and his work on MacOS \cite{25MillerOnMacOS} all fall in the same category of random generation based black box fuzzing.
A couple of years later, KLEE was developed\cite{8KLEE} by Cadar et al. KLEE is a generation based white box fuzzing tool


Among The more popular ones are: AFL, AFL++, Clusterfuzz, oneFuzz
Google's fuzzer for chromioum ClusterFuzz
OneFuzz from Microsoft 

%\subsection{AFL++}
%\subsection{KLEE}
%\subsection{ClusterFuzz}



%deze controleert dat de sat is still valid by using another solver as oracle (bij het annoteren van T/F values in the init pool)
% de eerste gebruikt een oracle bij het controleren of dat de equi transformations het model niet breken

\subsection{Types of bugs} 
Depending on what the output is of the fuzzer we can classify the types of bugs, as done in a recent paper\cite{1mansur2020detecting} by Mansur: crashes, wrongly satisfied, wrongly unsatisfied or hanging. With some of these bugs being less acceptable then others 
For example, as a recent paper\cite{1mansur2020detecting} by Mansur et al. describes, a crash for a constraint programming language (CP) is preferred over a wrongly unsatisfied model, since there is no way for the user to know that the solver failed (except for differentiation testing, more on that later). Meaning that the user will treat the result (wrongly) as correct compare this to a crash were it is clear that something went wrong.
With hanging PUT's the user can not draw incorrect conclusions and with wrongly satisfied models the user can check the model's instances and evaluate the result before using it further. This is due to the fact that problems are frequently np-hard meaning they are easy to confirm but hard to solve. For practical reasons we will later change the undecidable and or hanging PUT's into timeouts. We know that the types of bugs can be classified in more detail, for example crashes into buffer overflows, invalid memory addressing and so on, but we choose to stay with a more general overview for now. An interesting classification to be added is the knowledge whether or not the bug is in the parser or not, as the authors of "Semantic Fuzzing with Zest"\cite{22SemanticFuzzing} would classify, is the bug in the syntactical or in the semantical part of the program?


\subsection{oracle (what is a crash)}
The oracle problem describes the issue of telling if a PUT's output was, given the input, correct or not or as said in "The Oracle Problem in Software Testing: A Survey"\cite{10barr2014oracleProblem} 
\begin{quote}
	"Given an input for a system, the challenge of distinguishing the corresponding desired, correct behavior from potentially incorrect behavior is called the test oracle problem."
\end{quote} by Barr et al.
In their paper they discuss four categories: specified test oracles, derived test oracles, implicit test oracles, and the absence of test oracle. The biggest category would be the specified test oracles which contains all the possible encoding of specifications like modeling languages UML, Event-B and more. Their derived test oracles classification contains all forms of knowledge obtained from documentation on how the program should work or previous versions of the program. The last two oracles categories come down to the use of knowing that crashes are always unwanted and the human oracle like crowdsourcing respectively.

\subsection{Handling the oracle problem}
Although the approach of by Bugariu and M\"uller in "Automatically testing string solvers"\cite{9bugariu2020automaticallyTestingStringSolvers} falls in the first category mentioned above, their approach is innovative. While most fuzzers either use crashes or differential testing (more on that later) to find bugs, they know the  (un)satisfiability of their formulas by the way of they are constructed. For satisfiable formulas they generate trivial formulas and then by satisfiability preserving transformations increase the complexity and for unsatisfiable formulas they use $\neg$ A $\land$ A', with A' being a equivalent formula of A, to create the trivial unsatisfiable formulas. To increase the complexity of those trivial formulas, they again depend on satisfiability preserving transformation. This technique has also been applied to SMT solvers by Mansur et al. called STORM\cite{1mansur2020detecting} this with mutational input creation compared to the previous generation based technique. In the paper the authors dissect all assertions into their sub-formulas and create an initial pool. In this pool the sub-formulas are checked if they satisfy or not and with this knowledge new formulas are created for the population pool with ground truth.

\subsection{Differential testing}
As mentioned above most fuzzers use either crashes to detect that the PUT has failed to provide a correct output or in cases where possible use differential testing. This last one uses a single or multiple analogue programs to test if the PUt gave the same output as the analogue programs. neither techniques is complete: crash based fuzzing can not detect wrong outputs and differential testing can not catch bugs that also occur in the analogue programs.




%somewhere a ref to later chapter input simplification (minimisation differantioation)


\section{Conclusion}
The final section of the chapter gives an overview of the important results
of this chapter. This implies that the introductory chapter and the
concluding chapter don't need a conclusion.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
