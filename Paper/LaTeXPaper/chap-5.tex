\chapter{Implementation}
\label{cha:5:impl}
\label{impl:Intro}
In this chapter we will discuss how we build our fuzzers, what issues we had to circumvent and how we did that. Starting off with how we got our seeds to fuzz upon, to then discuss how we implemented the three techniques to finally end with how we deobfuscated the found bugs.

\section{Software versions used}
\label{impl:softwareVersion}
Throughout this paper we used CPMpy\footnote{\url{https://github.com/CPMpy/cpmpy}} version V0.9.9 (commit \href{https://github.com/CPMpy/cpmpy/commit/e79b3afedc934a9437c2ddb3a9f54d7e2d7bd3ee}{e79b3af}), unless specified otherwise, this version was chosen as it was the latest release version at the time of testing the first technique. All techniques were developed in Python 3.8, the MiniZinc solvers came with MiniZinc Python\footnote{\url{https://github.com/MiniZinc/minizinc-python}} release version 0.7.0 (commit \href{https://github.com/MiniZinc/minizinc-python/commit/a195cf63fcfbc98665d70ab64efb5424db25bd7e}{a195cf6}). For the proprietary solver Gurobi\footnote{\url{https://www.gurobi.com/}}, we used its Python version 9.5.2 with an academic license. 
%problems Gurobi outputting Not_run this was a trial problem, with the academic version does not occure
Originally, we did try to utilize the trial version to ease possible reproducibility, but the restrictions on the complexity of the problems became a hindrance which resulted in us moving to an academic license. For the other versions of the solvers, we used the ones included in the already mentioned packages, except for MiniZinc’s transformations to Google’s OR-Tools\footnote{\url{https://github.com/google/or-tools}}, there we had to install OR-Tools for MiniZinc manually, which we did using release version 9.3.10497 (commit \href{https://github.com/google/or-tools/commit/49b6301e1e1e231d654d79b6032e79809868a70e}{49b6301}).


\section{Obtaining seeds}
\label{impl:obtainingSeeds}
As discussed in a previous section (Section \ref{fuzzing:generationMutation}) generating new inputs is significantly harder than mutation, but with the latter one we require a diverse set of seed files. Fortunately, the CPMpy team made a lot of documentation and examples on how to model problems in their language. Ranging from easy examples to teach the language to advanced examples in order to showcase certain features. 
At the moment of writing most examples are found in the main branch and some extras can be found on the “csplib” branch\footnote{\url{https://github.com/CPMpy/cpmpy/tree/csplib}} waiting to be merged with the main branch. We downloaded a copy of those branches on Tuesday 27th of September to be used as future seed files. 

A second source of seeds files came from Hakan Kjellerstrand a retired software developer and independent researcher from Sweden which was found while reading “Model-Based Algorithm Configuration with Adaptive Capping and Prior Distributions” \cite{18bleukx2022model}. Mr. Kjellerstrand has a big repository\footnote{\url{https://github.com/hakank/hakank/tree/master/cpmpy}} full of problem models which he solves in multiple ways, including CPMpy. We obtained a copy of all his CPMpy examples on Tuesday 27th of September to top off our collection of future seed files.


After that we ran all seeds to test that the non-modified seeds do not crash on their own and noticed that most examples ran in less than a minute. The handful of examples that did run long were left out or were simplified to gain a speed up while solving them. Knowing that all seeds are capable of being run in a minute helps us avoid the halting problem. If a modified seed starts running significantly longer than a minute we can start investigating that seed for a potential bug. A final change we made to the future seed files is extracting the model from each file found on the repositories. We did this for a couple of reasons: some files had a loop around the solve instruction combined with small changes or had multiple problems in one file, this gave us a separate model for each found problem. In order to extract these constraints, we temporarily modified CPMpy to extract the created model, variables and constraints included, each time solve was called, this resulted in over nine thousand problem models which we will use as our seed files.


%We extracted our seeds twice, a first time where we extract our model without any flattening of the constraints and a second time where we did flatten the constraints. While building up a model of the problem CPMpy allows for arbitrary complex compositions of constraints resulting in a nested tree of constraints. However, not all solvers allow this nested tree as described by the documentations of CPMpy. It is for that reason that CPMpy flattens the constraints to what they call ‘normal forms’ as the similar definition of SAT but with a disclaimer that this does not exits to their knowledge with which we agree with, with this flattened form CPMpy can directly call the solvers or do some changes for the solver interface on the flattened constraints to then send it to the solver. The reason we extracted our seeds with and without a flattening process is that \todo{aanvullen na vraag} a flattened version and the reason we did is without will become clear in the next section.

\section{Modifying STORM into CTORM}
\label{impl:modifyingSTROM}
Our first technique of finding bugs is heavily based on STORM which we shortly discussed before in Section \ref{fuzzing:testingWithFuzzers}. Instead of searching for SMT bugs, here we want to be able to find bugs in constraint programming languages and specifically in CPMpy. We downloaded STORM from the repository\footnote{\url{https://github.com/Practical-Formal-Methods/storm}} on Tuesday 27th September.
%https://github.com/Practical-Formal-Methods/storm/commit/55d091624523a0544112ffc339fe81103b3daa2b
The original plan was to convert our seeds to FlatZinc using the MiniZinc API provided by CPMpy to then convert that to SMT-LIB \cite{72bofill2010system} using Miquel Bofill et al.’s fzn2smt-tool to then be able to use STORM as it was built originally. Unfortunately (and a bit predictable), this way of working did not work out. On top of fzn2smt being more than a decade old, the multiple transformation layers that could introduce conversion bugs and the unclear way back from SMT-LIB prevented this path from being investigated by us.

Therefore, we decided to refactor STORM to fit CPMpy and name the technique CTORM for CPMpy-STORM. To change STORM into CTORM, we needed to rewrite the detection, labeling and construction of (sub)constraints, this refactoring did come with some downsides, some features of STORM no longer work such as incremental solving or the input obfuscation that was built-in. A bigger downside came with the refactoring of the negation function of STORM, as CPMpy is still in active development and 
the negation not always being implemented already, this was felt while trying to negate global constraints. I.e., when trying to invert (sub)constraints which include \texttt{alldifferent([var1, var2, var3])} using CPMpy, it crashed, this is of course a bug (more specifically not yet implemented) in CPMpy but used by the fuzzer. So here we had the choice of adding the missing negation of global constraints to CPMpy or to limit our fuzzer to not use the missing features. We choose to limit the fuzzer, since we are trying to detect bugs in CPMpy with different tools and extending the language ourselves goes out of scope of this thesis. 

%remove?
%The resulting limitation on our fuzzer only influences the speed of generating new constraints and it can theoretically now get stuck but this has not happened yet, so we believe it to be a acceptable limitation.


We gave only non-flattened inputs to this solver since both STORM and CTORM used a recursive process to get all subformulas because we wanted to change as little as possible to the inner workings of CTORM compared to STORM. The flattening process in CPMpy is used to convert the potential tree-like constraint structures to a flattened list of constraints which the solver can handle. 
In order to get those non-flattened inputs, we hijacked the flatten process of CPMpy to return all subformulas before returning the flattened constraints, this gave access to the more convoluted subformulas to use in the next steps of CTORM. This flattening process was done before any modifications were made, so in the eyes of the fuzzer it got flattened seeds but with the knowledge of some more complex constraints just like STORM and CTORM does. For each input CTORM combines 100 new constraints built from the existing constraints, this is repeated to create a hundred models to then check if the result matched with the original output in CPMpy.
%\todo{optional image of this CTORM processes?}

%DUMB ideas:
%Translating seeds from solver X to solver Y 
%option 1 hardcode default solver of CPMpy to Y, less good modifying the language is something we want to avoid. May also not work when solver is hard coded in the seed.
%Option 2 interpret the seed and make changes so that the solver Y is run. Bit trickier as you cannot see the difference between model.solve() and solver.solve() because model and solver are variables.
%
%nested functions with global function inside are giving problems


\section{Metamorphic testing}
\label{impl:Meta}
While CTORM was quite autonomous, metamorphic testing did take one step back to manual work, as this technique requires some metamorphic transformations, these transformations take a (or multiple) constraint(s) of our seed problems and change them repeatedly while keeping the (un)satisfiability the same. To then test if the original seed problem gives the same result as our modified problem, as discussed in Subsection \ref{fuzzing:MetamorphicTesting}. 

With papers such as \cite{50akgun2018metamorphic, 49usman2020testmc, 43YinYang} and others giving us inspiration, we came up with but not limited to the following 30 metamorphic relations. Replacing global constraints like \texttt{alldifferent([var1, var2, var3])} to their decomposition \texttt{var1!=var2 and var1!=var3 and var2!=var3}. Adding futile variables to global constraints such as \texttt{allequal([var4, var5])} by copying variables which did not limit (or restrict) the existing solution-space. We did this too for other more basic operations such as “and”, “or”, “xor”, “->” (implication), all forms of comparisons, min, max and others. We also included metamorphic relations proposed by the authors of “Validating SMT Solvers via Semantic Fusion” \cite{43YinYang}, those being semantic fusion for addition, subtraction, multiplication, and, or, xor and the comparisons. All analog to the example given in Subsection \ref{fuzzing:SemanticFusion}. 

We also linked multiple (sub)constraints of the problem to each other and replaced comparisons by other equivalent comparisons. Lastly, we also added new constraints which were independent of the original problem only to get in the way of the seed problem or be used in other metamorphic relations.

All these metamorphic relations individually were quite simple and should be handled easily by the flattening process, other CPMpy processes or by the solvers, but by combining multiple relations at random we were able to create more complex constraints that were not always handled correctly. Finally, we should note that while finishing this thesis Jo Devriendt found a bug within the metamorphic tester that made cyclic expressions possible which will crash CPMpy, which was swiftly fixed in the tester. More information about this bug can be found in the CPMpy bug report number \href{https://github.com/CPMpy/cpmpy/issues/163}{163}.

\section{Differential testing}
\label{impl:diff}
With differential testing we stepped a bit further away from the fuzzing world since we did not edit the input files.
With this last technique we put the (sub)solvers against each other, if solver A said that the problem is unsatisfiable and all other solvers said that the same problem was satisfiable we can say that we found a bug and that the bug was most likely to be in solver A, which differs from the previous techniques where we knew the correct solution in advance.

%This test could easily be integrated in one (or both) previous fuzzers 
% since this can be integrated as an extra check in the fuzzers. 

The way this tester was written is quite simple, we tested a given input on multiple solvers and searched if there was any difference in outputs or on the number of solutions provided in the results.
%ony 2 solvers
However, we discovered that only two solvers, namely “ortools” and “gurobi”-solvers, are able to search all solutions for problem models that contain global constraints. More solvers are available to find all solutions for SAT problems but 
the main objective of this thesis is to find CP-related bugs. The limitation to only two solvers results in only being able to compare two different implementations and has a risk of overlapping bugs. Preferably, we would have three or more solvers to be able to compare between them and also be able to automatically show us which of the solvers is likely to be wrong. In the future more solvers will be available within CPMpy with the capability of finding all solutions, but right now these tests are performed with two solvers. Small note, a limit of 100 solutions was put in place, otherwise finding solutions would take an unreasonable amount of time. 

When we look at only searching for one solution for a given example, we had more than enough solvers to compare between. Most of the times 14 solvers were compared and we never got lower than 6 solvers, this variation is due to the features used in the given problem the amount of solvers changed. If a problem happens to not use any global constraints the SAT-solvers may be able to solve them as well allowing us to compare between up to 36 solvers. Given that we only search for a single solution we are limited in what we can compare. For example, if solver A says that it found a satisfiable problem with the solution having values A' and solver B says the same but with different values B' (with A $\neq$ B ) the only thing we can compare is that both solvers output the same “satisfiable”. Since both solution A' and B' can be different solutions to the same problem.


\section{Detecting the cause of the bug}
\label{impl:DetectingCause}
As described in Chapter \ref{inputReduction:intro} finding a bug is one part of the problem, the other is finding which part of the inputs causes the bug. Submitting a complex input would result in a lot of work for the development team in order to find the cause. To avoid this situation, we used a Minimal Unsatisfiable Subset finder created by the CPMpy-team. The first version we used can be found in the advanced folder\footnote{\url{https://github.com/CPMpy/cpmpy/blob/master/examples/advanced/musx.py}} of the examples of CPMpy version 0.9.9. It was limited to finding MUS with the OR-Tools solver only, which caused us to write our own programs to deobfuscate the inputs based on this MUS-finder. 

We created a program to simplify inputs as described in Subsection \ref{inputReduction:Simplifying} and a program to isolate inputs as detailed in Subsection \ref{inputReduction:Isolation}. Nevertheless, we ended up only using the simplification technique due to not wanting to report a crucial part of the bug in one issue and reporting the difference between two inputs in the next issue. The already mentioned time penalty was not noticeable due to the inputs not being enormous. Both the programs we created did not give a minimal input back but did minimize the number of constraints, the difference being that some constraints could contain non-crucial parts of the bug. For example, a constraint “[15][variable1](...)” would give the exact same bug as constraint “[0, 0, 0, 0, 0, 0, 0, 0, 0, ... 0, 15][variable1](...)” but be clearer for the developer. 

To achieve a minimal input, we were planning to update our previously created programs, but CPMpy version 0.9.10 came out and contained a better MUS-finding program\footnote{\url{https://github.com/CPMpy/cpmpy/blob/master/cpmpy/tools/mus.py}} suitable for finding MUS for all available solvers, which was important since not all (sub)solvers would agree on the problem being satisfiable or not.


Nevertheless letting a solver search for an unsatisfiable subset in what it thinks is a satisfiable problem does not work, as it will never find an unsatisfiable subset for obvious reasons.
It was a similar case for crashes, for which we used a modified version of our simplification program. For obvious reasons the MUS-finders of CPMpy did not work for crashing programs. Our simplification program would test if the model still had a crash with some missing constraints or not and continue analog to the MUS version. It would again result in a minimal number of constraints but not in a minimal model. To get a minimal model some manual work was needed but that was not a significant amount of work. 

%\section{Tested solver}
%As a small side note not all solver within CPMpy are that usseefull if we wnat to find bug in CP languages 
%\todo{focused solvers}
%pySSD + PYsat: graph maker minder relevant omdat we CP fouten willen vinden 


\section{Conclusion}
\label{impl:conclusion}
In this chapter we discussed which software was used in tandem with the version’s numbers and GitHub repositories where applicable. We showed how we turned CPMpy’s and Hakan’s examples into the seed files we then used for our three techniques. After that, we specified how we implemented our techniques to find bugs, this being the modification of STORM into CTORM, the creation of metamorphic relations and the comparison of the output of (sub)solvers in the differential testing. To finally end with the tools used to minimize the input files once a bug was found.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
